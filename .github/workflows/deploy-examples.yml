# Builtin Deploy Examples (Baremetal)
#
# Canonical baremetal example workflow:
# - hello-tdx on tiny
# - private-llm on llm (OpenAI-compatible smoke test, production only)

name: Builtin Deploy Examples (Baremetal)

on:
  workflow_call:
    inputs:
      cp_url:
        required: false
        type: string
        default: "https://app.easyenclave.com"
      cp_expected_git_sha:
        required: false
        type: string
        default: ""
      cp_expected_boot_id:
        required: false
        type: string
        default: ""
      cp_admin_token:
        required: false
        type: string
        default: ""
      source_ref:
        required: false
        type: string
        default: ""
      hello_tdx_image:
        required: false
        type: string
        default: ""
      private_llm_ollama_image:
        required: false
        type: string
        default: ""
      private_llm_model_loader_image:
        required: false
        type: string
        default: ""
      easyenclave_env:
        required: false
        type: string
        default: "staging"
  workflow_dispatch:
    inputs:
      cp_url:
        description: "Control plane URL"
        required: false
        default: "https://app.easyenclave.com"
      cp_expected_git_sha:
        description: "Expected control-plane git SHA from /health (optional)"
        required: false
        default: ""
      cp_expected_boot_id:
        description: "Expected control-plane boot_id from /health (optional)"
        required: false
        default: ""
      cp_admin_token:
        description: "Optional admin bearer token for trust bootstrap"
        required: false
        default: ""
      source_ref:
        description: "Git ref to checkout"
        required: false
        default: ""
      hello_tdx_image:
        description: "Optional hello-tdx image override (digest ref)"
        required: false
        default: ""
      private_llm_ollama_image:
        description: "Optional private-llm ollama image override (digest ref)"
        required: false
        default: ""
      private_llm_model_loader_image:
        description: "Optional private-llm model-loader image override (digest ref)"
        required: false
        default: ""
      easyenclave_env:
        description: "Environment profile (staging or production)"
        required: false
        default: "staging"

concurrency:
  group: easyenclave-deploy-examples-baremetal-${{ github.run_id }}
  cancel-in-progress: true

env:
  CP_URL: ${{ inputs.cp_url || github.event.inputs.cp_url || 'https://app.easyenclave.com' }}
  CP_EXPECTED_GIT_SHA: ${{ inputs.cp_expected_git_sha || github.event.inputs.cp_expected_git_sha || '' }}
  CP_EXPECTED_BOOT_ID: ${{ inputs.cp_expected_boot_id || github.event.inputs.cp_expected_boot_id || '' }}
  CP_ADMIN_TOKEN: ${{ inputs.cp_admin_token || github.event.inputs.cp_admin_token || '' }}
  HELLO_TDX_IMAGE: ${{ inputs.hello_tdx_image || github.event.inputs.hello_tdx_image || '' }}
  PRIVATE_LLM_OLLAMA_IMAGE: ${{ inputs.private_llm_ollama_image || github.event.inputs.private_llm_ollama_image || '' }}
  PRIVATE_LLM_MODEL_LOADER_IMAGE: ${{ inputs.private_llm_model_loader_image || github.event.inputs.private_llm_model_loader_image || '' }}
  EASYENCLAVE_ENV: ${{ inputs.easyenclave_env || github.event.inputs.easyenclave_env || 'staging' }}

jobs:
  deploy-builtin-baremetal:
    name: Deploy builtin apps (baremetal)
    runs-on: [self-hosted, tdx]
    steps:
      - name: Wait for control plane health
        shell: bash
        run: |
          set -euo pipefail
          echo "Waiting for $CP_URL/health git_sha=${CP_EXPECTED_GIT_SHA:-<any>} boot_id=${CP_EXPECTED_BOOT_ID:-<any>}"
          deadline=$(( $(date +%s) + 1200 ))
          while :; do
            body="$(curl -fsS "$CP_URL/health" 2>/dev/null || true)"
            if [ -z "${body:-}" ]; then
              now="$(date +%s)"
              if [ "$now" -ge "$deadline" ]; then
                echo "::error::Timed out waiting for control plane health"
                exit 1
              fi
              sleep 5
              continue
            fi

            sha="$(echo "${body:-}" | jq -r '.git_sha // empty' 2>/dev/null || true)"
            boot_id="$(echo "${body:-}" | jq -r '.boot_id // empty' 2>/dev/null || true)"

            ready=1
            if [ -n "${CP_EXPECTED_GIT_SHA:-}" ] && [ "${sha:-}" != "${CP_EXPECTED_GIT_SHA}" ]; then
              ready=0
            fi
            if [ -n "${CP_EXPECTED_BOOT_ID:-}" ] && [ "${boot_id:-}" != "${CP_EXPECTED_BOOT_ID}" ]; then
              ready=0
            fi

            if [ "$ready" -eq 1 ]; then
              echo "Control plane ready: git_sha=${sha:-unknown} boot_id=${boot_id:-unknown}"
              break
            fi

            now="$(date +%s)"
            if [ "$now" -ge "$deadline" ]; then
              echo "::error::Timed out waiting for control plane identity match."
              echo "::error::Expected git_sha='${CP_EXPECTED_GIT_SHA:-<any>}' boot_id='${CP_EXPECTED_BOOT_ID:-<any>}'"
              echo "::error::Last seen git_sha='${sha:-<none>}' boot_id='${boot_id:-<none>}'"
              echo "Last /health:"
              echo "${body:-<empty>}"
              exit 1
            fi
            sleep 5
          done

      - uses: actions/checkout@v4
        with:
          ref: ${{ inputs.source_ref || github.event.inputs.source_ref || github.sha }}

      - name: Register hello-tdx
        uses: ./.github/actions/register-app
        with:
          name: hello-tdx
          description: "Hello TDX - Example app demonstrating TDX attestation"
          control_plane_url: ${{ env.CP_URL }}

      - name: Register private-llm
        if: ${{ env.EASYENCLAVE_ENV == 'production' }}
        uses: ./.github/actions/register-app
        with:
          name: private-llm
          description: "Private LLM - Ollama running in a TDX enclave"
          control_plane_url: ${{ env.CP_URL }}

      - name: Prepare hello-tdx compose (optional release image override)
        id: hello_compose
        shell: bash
        run: |
          set -euo pipefail
          src="examples/hello-tdx/docker-compose.yml"
          out="${RUNNER_TEMP}/hello-tdx.compose.yml"
          cp "$src" "$out"
          if [ -n "${HELLO_TDX_IMAGE:-}" ]; then
            if ! grep -qE 'image:[[:space:]]*hashicorp/http-echo(@sha256:[0-9a-f]{64}|:latest)' "$out"; then
              echo "::error::Expected hello-tdx image line not found in $src"
              exit 1
            fi
            escaped="$(printf '%s' "${HELLO_TDX_IMAGE}" | sed -e 's/[&|]/\\&/g')"
            sed -i -E "s|image:[[:space:]]*hashicorp/http-echo(@sha256:[0-9a-f]{64}|:latest)|image: ${escaped}|g" "$out"
          fi
          echo "compose_file=$out" >> "$GITHUB_OUTPUT"

      - name: Deploy hello-tdx
        id: deploy_hello
        uses: ./.github/actions/deploy-example
        with:
          app_name: hello-tdx
          compose_file: ${{ steps.hello_compose.outputs.compose_file }}
          service_name: hello-tdx
          health_endpoint: /
          control_plane_url: ${{ env.CP_URL }}
          node_size: tiny
          allowed_clouds: baremetal
          allowed_datacenters: baremetal:github-runner

      - name: Prepare private-llm compose (optional release image overrides)
        if: ${{ env.EASYENCLAVE_ENV == 'production' }}
        id: llm_compose
        shell: bash
        run: |
          set -euo pipefail
          src="examples/private-llm/docker-compose.yml"
          out="${RUNNER_TEMP}/private-llm.compose.yml"
          cp "$src" "$out"

          if [ -n "${PRIVATE_LLM_OLLAMA_IMAGE:-}" ]; then
            if ! grep -qE 'image:[[:space:]]*ollama/ollama(@sha256:[0-9a-f]{64}|:latest)' "$out"; then
              echo "::error::Expected private-llm ollama image line not found in $src"
              exit 1
            fi
            escaped="$(printf '%s' "${PRIVATE_LLM_OLLAMA_IMAGE}" | sed -e 's/[&|]/\\&/g')"
            sed -i -E "s|image:[[:space:]]*ollama/ollama(@sha256:[0-9a-f]{64}|:latest)|image: ${escaped}|g" "$out"
          fi

          if [ -n "${PRIVATE_LLM_MODEL_LOADER_IMAGE:-}" ]; then
            if ! grep -qE 'image:[[:space:]]*curlimages/curl(@sha256:[0-9a-f]{64}|:latest)' "$out"; then
              echo "::error::Expected private-llm model-loader image line not found in $src"
              exit 1
            fi
            escaped="$(printf '%s' "${PRIVATE_LLM_MODEL_LOADER_IMAGE}" | sed -e 's/[&|]/\\&/g')"
            sed -i -E "s|image:[[:space:]]*curlimages/curl(@sha256:[0-9a-f]{64}|:latest)|image: ${escaped}|g" "$out"
          fi

          echo "compose_file=$out" >> "$GITHUB_OUTPUT"

      - name: Deploy private-llm
        if: ${{ env.EASYENCLAVE_ENV == 'production' }}
        id: deploy
        uses: ./.github/actions/deploy-example
        with:
          app_name: private-llm
          compose_file: ${{ steps.llm_compose.outputs.compose_file }}
          service_name: private-llm
          health_endpoint: /
          control_plane_url: ${{ env.CP_URL }}
          agent_admin_password: ${{ secrets.AGENT_ADMIN_PASSWORD }}
          github_owner: ${{ github.repository_owner }}
          node_size: llm
          allowed_clouds: baremetal
          allowed_datacenters: baremetal:github-runner

      - name: Set up Python
        if: ${{ env.EASYENCLAVE_ENV == 'production' }}
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install SDK and OpenAI client
        if: ${{ env.EASYENCLAVE_ENV == 'production' }}
        run: pip install ./sdk/ openai

      - name: Smoke test private-llm (OpenAI-compatible)
        if: ${{ env.EASYENCLAVE_ENV == 'production' }}
        env:
          SERVICE_URL: ${{ steps.deploy.outputs.service_url }}
          EASYENCLAVE_URL: ${{ env.CP_URL }}
        shell: bash
        run: |
          set -euo pipefail
          python3 - <<'PY'
          import os
          import time

          import httpx
          import httpx as _httpx
          from easyenclave import EasyEnclaveClient
          from openai import OpenAI

          MODEL = "smollm2:135m"
          CHAT_PATH = "/v1/chat/completions"
          CHAT_BODY = {
              "model": MODEL,
              "messages": [{"role": "user", "content": "Say hello in one sentence."}],
          }
          TIMEOUT = 300
          RETRY_INTERVAL = 15

          service_url = os.environ["SERVICE_URL"].rstrip("/")
          easyenclave_url = os.environ["EASYENCLAVE_URL"].rstrip("/")

          def wait_ready(label, fn):
              deadline = time.monotonic() + TIMEOUT
              last_error = RuntimeError("unknown")
              while True:
                  try:
                      content = fn()
                      if content and content != "null":
                          print(f"[{label}] OK: {content}")
                          return
                  except Exception as exc:
                      last_error = exc
                  if time.monotonic() >= deadline:
                      raise RuntimeError(
                          f"[{label}] FAIL: not ready after {TIMEOUT}s â€” {last_error}"
                      )
                  print(f"[{label}] Model not ready, retrying in {RETRY_INTERVAL}s...")
                  time.sleep(RETRY_INTERVAL)

          def extract_content(resp: httpx.Response) -> str:
              data = resp.json()
              return data["choices"][0]["message"]["content"]

          def direct_call():
              url = f"{service_url}{CHAT_PATH}"
              with httpx.Client(timeout=60, headers={"user-agent": "EasyEnclave-Test/1.0"}) as c:
                  resp = c.post(url, json=CHAT_BODY)
                  resp.raise_for_status()
                  return extract_content(resp)

          ee = EasyEnclaveClient(easyenclave_url, verify=False)
          llm = ee.service("private-llm")

          def proxy_call():
              resp = llm.post(CHAT_PATH, json=CHAT_BODY, timeout=60)
              resp.raise_for_status()
              return extract_content(resp)

          def _strip_bot_headers(req: _httpx.Request):
              req.headers["user-agent"] = "EasyEnclave-Test/1.0"
              for key in [k for k in req.headers if k.lower().startswith("x-stainless-")]:
                  del req.headers[key]

          openai_client = OpenAI(
              base_url=f"{llm.base_url.rstrip('/')}/v1",
              api_key="unused",
              http_client=_httpx.Client(
                  verify=False,
                  event_hooks={"request": [_strip_bot_headers]},
              ),
          )

          def openai_call():
              completion = openai_client.chat.completions.create(
                  model=MODEL,
                  messages=[{"role": "user", "content": "Say hello in one sentence."}],
                  timeout=60,
              )
              return completion.choices[0].message.content or ""

          wait_ready("direct", direct_call)
          wait_ready("proxy", proxy_call)
          wait_ready("openai", openai_call)
          PY

      - name: Resolve admin token for hello teardown
        id: teardown_auth
        if: always()
        shell: bash
        env:
          CP_ADMIN_TOKEN: ${{ env.CP_ADMIN_TOKEN || secrets.CP_ADMIN_TOKEN }}
          CP_ADMIN_PASSWORD: ${{ secrets.CP_ADMIN_PASSWORD }}
        run: |
          set -euo pipefail
          token="$(echo "${CP_ADMIN_TOKEN:-}" | xargs)"
          if [ -z "$token" ] && [ -n "${CP_ADMIN_PASSWORD:-}" ]; then
            payload="$(jq -cn --arg p "${CP_ADMIN_PASSWORD}" '{password: $p}')"
            token="$(curl -sS -X POST "$CP_URL/admin/login" -H 'Content-Type: application/json' -d "$payload" | jq -r '.token // empty' 2>/dev/null || true)"
          fi
          if [ -z "$token" ]; then
            echo "::warning::Could not resolve admin token for hello-tdx teardown."
            exit 0
          fi
          echo "::add-mask::$token"
          echo "admin_token=$token" >> "$GITHUB_OUTPUT"
          echo "Resolved admin token for hello-tdx teardown."

      - name: Teardown hello-tdx baremetal deployment
        if: always() && steps.deploy_hello.outcome == 'success'
        shell: bash
        env:
          ADMIN_TOKEN: ${{ steps.teardown_auth.outputs.admin_token }}
          AGENT_ID: ${{ steps.deploy_hello.outputs.agent_id }}
        run: |
          set -euo pipefail
          if [ -z "${AGENT_ID:-}" ]; then
            echo "::warning::No hello-tdx agent_id output; skipping teardown."
            exit 0
          fi
          if [ -z "${ADMIN_TOKEN:-}" ]; then
            echo "::warning::Missing admin token; cannot teardown hello-tdx deployment."
            exit 0
          fi

          undeploy_code="$(curl -sS -o /tmp/hello-undeploy.json -w "%{http_code}" \
            -X POST "$CP_URL/api/v1/agents/${AGENT_ID}/undeploy" \
            -H "Authorization: Bearer ${ADMIN_TOKEN}" \
            -H 'Accept: application/json' || true)"
          if [ "$undeploy_code" -lt 400 ]; then
            echo "hello-tdx undeploy succeeded for agent ${AGENT_ID}."
          else
            echo "::warning::hello-tdx undeploy failed (HTTP $undeploy_code): $(cat /tmp/hello-undeploy.json)"
          fi

          cleanup_body="$(jq -cn --arg reason "ci:baremetal-hello-post-test-teardown" '{dry_run:false, reason:$reason}')"
          cleanup_code="$(curl -sS -o /tmp/hello-cleanup.json -w "%{http_code}" \
            -X POST "$CP_URL/api/v1/admin/agents/${AGENT_ID}/cleanup" \
            -H "Authorization: Bearer ${ADMIN_TOKEN}" \
            -H 'Content-Type: application/json' \
            -d "$cleanup_body" || true)"
          if [ "$cleanup_code" -lt 400 ]; then
            echo "hello-tdx cleanup succeeded for agent ${AGENT_ID}."
          else
            echo "::warning::hello-tdx cleanup failed (HTTP $cleanup_code): $(cat /tmp/hello-cleanup.json)"
          fi
