# Builtin Deploy Examples (Baremetal)
#
# Canonical baremetal example workflow:
# - hello-tdx on tiny
# - private-llm on llm (OpenAI-compatible smoke test)

name: Builtin Deploy Examples (Baremetal)

on:
  workflow_call:
    inputs:
      cp_url:
        required: false
        type: string
        default: "https://app.easyenclave.com"
      cp_expected_git_sha:
        required: false
        type: string
        default: ""
      cp_admin_token:
        required: false
        type: string
        default: ""
      source_ref:
        required: false
        type: string
        default: ""
      hello_tdx_image:
        required: false
        type: string
        default: ""
      private_llm_ollama_image:
        required: false
        type: string
        default: ""
      private_llm_model_loader_image:
        required: false
        type: string
        default: ""
  workflow_dispatch:
    inputs:
      cp_url:
        description: "Control plane URL"
        required: false
        default: "https://app.easyenclave.com"
      cp_expected_git_sha:
        description: "Expected control-plane git SHA from /health (optional)"
        required: false
        default: ""
      cp_admin_token:
        description: "Optional admin bearer token for trust bootstrap"
        required: false
        default: ""
      source_ref:
        description: "Git ref to checkout"
        required: false
        default: ""
      hello_tdx_image:
        description: "Optional hello-tdx image override (digest ref)"
        required: false
        default: ""
      private_llm_ollama_image:
        description: "Optional private-llm ollama image override (digest ref)"
        required: false
        default: ""
      private_llm_model_loader_image:
        description: "Optional private-llm model-loader image override (digest ref)"
        required: false
        default: ""

concurrency:
  group: easyenclave-deploy-examples-baremetal-${{ github.run_id }}
  cancel-in-progress: true

env:
  CP_URL: ${{ inputs.cp_url || github.event.inputs.cp_url || 'https://app.easyenclave.com' }}
  CP_EXPECTED_GIT_SHA: ${{ inputs.cp_expected_git_sha || github.event.inputs.cp_expected_git_sha || '' }}
  CP_ADMIN_TOKEN: ${{ inputs.cp_admin_token || github.event.inputs.cp_admin_token || '' }}
  HELLO_TDX_IMAGE: ${{ inputs.hello_tdx_image || github.event.inputs.hello_tdx_image || '' }}
  PRIVATE_LLM_OLLAMA_IMAGE: ${{ inputs.private_llm_ollama_image || github.event.inputs.private_llm_ollama_image || '' }}
  PRIVATE_LLM_MODEL_LOADER_IMAGE: ${{ inputs.private_llm_model_loader_image || github.event.inputs.private_llm_model_loader_image || '' }}

jobs:
  wait-control-plane:
    name: Wait for control plane
    runs-on: [self-hosted, tdx]
    steps:
      - name: Wait for control plane health
        shell: bash
        run: |
          set -euo pipefail
          echo "Waiting for $CP_URL/health"
          deadline=$(( $(date +%s) + 1200 ))
          while :; do
            body="$(curl -fsS "$CP_URL/health" 2>/dev/null || true)"
            if [ -z "${body:-}" ]; then
              now="$(date +%s)"
              if [ "$now" -ge "$deadline" ]; then
                echo "::error::Timed out waiting for control plane health"
                exit 1
              fi
              sleep 5
              continue
            fi

            if [ -z "${CP_EXPECTED_GIT_SHA:-}" ]; then
              echo "Control plane healthy (git_sha check skipped)"
              break
            fi

            sha="$(echo "${body:-}" | jq -r '.git_sha // empty' 2>/dev/null || true)"
            if [ -n "${sha:-}" ] && [ "$sha" = "$CP_EXPECTED_GIT_SHA" ]; then
              echo "Control plane ready: git_sha=$sha"
              break
            fi

            now="$(date +%s)"
            if [ "$now" -ge "$deadline" ]; then
              echo "::error::Timed out waiting for control plane git_sha=$CP_EXPECTED_GIT_SHA"
              echo "Last /health:"
              echo "${body:-<empty>}"
              exit 1
            fi
            sleep 5
          done

  register-apps:
    name: Register builtin apps
    runs-on: [self-hosted, tdx]
    needs: [wait-control-plane]
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ inputs.source_ref || github.event.inputs.source_ref || github.sha }}

      - name: Register hello-tdx
        uses: ./.github/actions/register-app
        with:
          name: hello-tdx
          description: "Hello TDX - Example app demonstrating TDX attestation"
          control_plane_url: ${{ env.CP_URL }}

      - name: Register private-llm
        uses: ./.github/actions/register-app
        with:
          name: private-llm
          description: "Private LLM - Ollama running in a TDX enclave"
          control_plane_url: ${{ env.CP_URL }}

  ensure-baremetal-capacity:
    name: Ensure baremetal capacity (tiny + llm)
    runs-on: [self-hosted, tdx]
    needs: [register-apps]
    permissions:
      contents: read
    env:
      ITA_API_KEY: ${{ secrets.INTEL_API_KEY }}
      CP_ADMIN_TOKEN: ${{ inputs.cp_admin_token || github.event.inputs.cp_admin_token || secrets.CP_ADMIN_TOKEN }}
      CP_ADMIN_PASSWORD: ${{ secrets.CP_ADMIN_PASSWORD }}
      CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
      CLOUDFLARE_ZONE_ID: ${{ secrets.CLOUDFLARE_ZONE_ID }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ inputs.source_ref || github.event.inputs.source_ref || github.sha }}

      - name: Validate required secrets
        shell: bash
        run: |
          set -euo pipefail
          if [ -z "${ITA_API_KEY:-}" ]; then
            echo "::error::Missing required secret/env: ITA_API_KEY"
            exit 1
          fi
          if [ -z "${CP_ADMIN_TOKEN:-}" ] && [ -z "${CP_ADMIN_PASSWORD:-}" ]; then
            echo "::warning::Neither CP_ADMIN_TOKEN nor CP_ADMIN_PASSWORD is set; baseline trust step can only pass if llm MRTD is already trusted."
          fi

      - name: Install Nix
        uses: DeterminateSystems/nix-installer-action@main

      - name: Build verity artifacts (required for tdx_cli vm new)
        shell: bash
        run: |
          set -euo pipefail
          cd infra/image
          nix develop --command make build

      - name: Cleanup stale Cloudflare agent resources (best effort)
        shell: bash
        run: |
          set -euo pipefail
          for v in CLOUDFLARE_API_TOKEN CLOUDFLARE_ACCOUNT_ID CLOUDFLARE_ZONE_ID; do
            if [ -z "${!v:-}" ]; then
              echo "::warning::Skipping Cloudflare cleanup: missing $v"
              exit 0
            fi
          done

          if [ -z "${CP_URL:-}" ]; then
            echo "::warning::Skipping Cloudflare cleanup: missing CP_URL"
            exit 0
          fi

          python3 scripts/cleanup_stale_tunnels.py || true
          python3 scripts/cleanup_stale_dns.py || true

      - name: Cleanup stale CI LLM VMs (best effort)
        shell: bash
        run: |
          set -euo pipefail
          bash ./scripts/cleanup_ci_llm_vms.sh || true

      - name: Trust measured llm baseline
        shell: bash
        run: |
          set -euo pipefail

          measurements="$(python3 infra/tdx_cli.py vm measure --size llm --json)"
          mrtd="$(echo "$measurements" | jq -r '.mrtd // empty')"
          if ! echo "$mrtd" | grep -Eq '^[0-9a-f]{96}$'; then
            echo "::error::Failed to measure valid llm baseline MRTD"
            echo "$measurements" | head -c 2000 || true
            exit 1
          fi

          already="$(curl -sS -H 'Accept: application/json' "$CP_URL/api/v1/trusted-mrtds" | jq -r --arg m "$mrtd" '[.trusted_mrtds[] | select((.mrtd // "") == $m)] | length' 2>/dev/null || echo 0)"
          if [ "${already:-0}" -gt 0 ]; then
            echo "LLM baseline already trusted: ${mrtd:0:16}..."
            exit 0
          fi

          admin_token="${CP_ADMIN_TOKEN:-}"
          if [ -z "${admin_token:-}" ] && [ -n "${CP_ADMIN_PASSWORD:-}" ]; then
            payload="$(jq -cn --arg p "${CP_ADMIN_PASSWORD}" '{password: $p}')"
            admin_token="$(curl -sS -X POST "$CP_URL/admin/login" -H 'Content-Type: application/json' -d "$payload" | jq -r '.token // empty' 2>/dev/null || true)"
          fi
          if [ -z "${admin_token:-}" ]; then
            echo "::error::LLM baseline ${mrtd:0:16}... is not trusted, and no admin token could be resolved (password login may be disabled)."
            echo "::error::Set CP_ADMIN_TOKEN for CI, or preseed this MRTD in trusted values."
            exit 1
          fi

          payload="$(jq -cn --arg m "$mrtd" --arg note "builtin-examples llm baseline" '{mrtd:$m, type:"agent", note:$note}')"
          code="$(curl -sS -o /tmp/trust-mrtd.json -w '%{http_code}' -X POST "$CP_URL/api/v1/admin/trusted-mrtds" -H "Authorization: Bearer ${admin_token}" -H 'Content-Type: application/json' -d "$payload" || true)"
          if [ "$code" -ge 400 ]; then
            echo "::error::Failed to trust llm MRTD (HTTP $code): $(cat /tmp/trust-mrtd.json)"
            exit 1
          fi
          echo "Trusted measured llm baseline MRTD: ${mrtd:0:16}..."

      - name: Ensure deployable baremetal tiny + llm agents exist
        shell: bash
        run: |
          set -euo pipefail
          ita_key="${ITA_API_KEY:-${INTEL_API_KEY:-}}"
          if [ -z "${ita_key:-}" ]; then
            echo "::error::Missing ITA_API_KEY (Intel Trust Authority API key); cannot boot agents."
            exit 1
          fi

          count_deployable() {
            local size="$1"
            curl -sSf "$CP_URL/api/v1/agents" | jq -r --arg ns "$size" '
              [.agents[]
                | select(.verified == true)
                | select((.health_status // "" | ascii_downcase) == "healthy")
                | select(((.status // "" | ascii_downcase) == "undeployed" or (.status // "" | ascii_downcase) == "deployed"))
                | select((.node_size // "" | ascii_downcase) == ($ns | ascii_downcase))
                | select((.datacenter // "" | ascii_downcase) == "baremetal:github-runner")
                | select(((.deployed_app // "") | startswith("measuring-enclave")) | not)
              ] | length'
          }

          ensure_capacity() {
            local size="$1"
            local required="$2"
            local max_boots="${3:-3}"
            local boots=0

            local count
            count="$(count_deployable "$size" || echo 0)"
            echo "Deployable baremetal $size candidates: $count (required: $required)"

            while [ "${count:-0}" -lt "$required" ]; do
              boots=$((boots + 1))
              if [ "$boots" -gt "$max_boots" ]; then
                echo "::error::Exceeded max $size boot attempts ($max_boots)"
                curl -sSf "$CP_URL/api/v1/agents" | jq -r --arg ns "$size" '
                  [.agents[] | select((.node_size // "" | ascii_downcase) == ($ns | ascii_downcase))
                    | {agent_id, status, health_status, verified, datacenter, deployed_app, vm_name, verification_error}]'
                exit 1
              fi

              echo "Booting a new baremetal $size agent VM..."
              python3 infra/tdx_cli.py vm new \
                --size "$size" \
                --cloud-provider baremetal \
                --availability-zone github-runner \
                --easyenclave-url "$CP_URL" \
                --intel-api-key "$ita_key" \
                --wait \
                >"/tmp/new_${size}_vm.json"
              cat "/tmp/new_${size}_vm.json" || true

              for i in {1..60}; do
                count="$(count_deployable "$size" || echo 0)"
                if [ "${count:-0}" -ge "$required" ]; then
                  break
                fi
                if [ $((i % 6)) -eq 0 ]; then
                  echo "Still waiting for deployable baremetal $size capacity... ($i/60)"
                fi
                sleep 5
              done

              count="$(count_deployable "$size" || echo 0)"
              echo "Deployable baremetal $size candidates now: $count (required: $required)"
            done
          }

          ensure_capacity tiny 1 2
          ensure_capacity llm 1 3

  deploy-hello-tdx:
    name: Deploy hello-tdx (baremetal tiny)
    runs-on: [self-hosted, tdx]
    needs: [ensure-baremetal-capacity]
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ inputs.source_ref || github.event.inputs.source_ref || github.sha }}

      - name: Prepare hello-tdx compose (optional release image override)
        id: hello_compose
        shell: bash
        run: |
          set -euo pipefail
          src="examples/hello-tdx/docker-compose.yml"
          out="${RUNNER_TEMP}/hello-tdx.compose.yml"
          cp "$src" "$out"
          if [ -n "${HELLO_TDX_IMAGE:-}" ]; then
            if ! grep -qE 'image:[[:space:]]*hashicorp/http-echo:latest' "$out"; then
              echo "::error::Expected hello-tdx image line not found in $src"
              exit 1
            fi
            escaped="$(printf '%s' "${HELLO_TDX_IMAGE}" | sed -e 's/[&|]/\\&/g')"
            sed -i -E "s|image:[[:space:]]*hashicorp/http-echo:latest|image: ${escaped}|g" "$out"
          fi
          echo "compose_file=$out" >> "$GITHUB_OUTPUT"

      - name: Deploy hello-tdx
        uses: ./.github/actions/deploy-example
        with:
          app_name: hello-tdx
          compose_file: ${{ steps.hello_compose.outputs.compose_file }}
          service_name: hello-tdx
          health_endpoint: /
          control_plane_url: ${{ env.CP_URL }}
          node_size: tiny
          allowed_clouds: baremetal
          allowed_datacenters: baremetal:github-runner

  deploy-private-llm:
    name: Deploy private-llm (baremetal llm)
    runs-on: [self-hosted, tdx]
    needs: [ensure-baremetal-capacity]
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ inputs.source_ref || github.event.inputs.source_ref || github.sha }}

      - name: Prepare private-llm compose (optional release image overrides)
        id: llm_compose
        shell: bash
        run: |
          set -euo pipefail
          src="examples/private-llm/docker-compose.yml"
          out="${RUNNER_TEMP}/private-llm.compose.yml"
          cp "$src" "$out"

          if [ -n "${PRIVATE_LLM_OLLAMA_IMAGE:-}" ]; then
            if ! grep -qE 'image:[[:space:]]*ollama/ollama:latest' "$out"; then
              echo "::error::Expected private-llm ollama image line not found in $src"
              exit 1
            fi
            escaped="$(printf '%s' "${PRIVATE_LLM_OLLAMA_IMAGE}" | sed -e 's/[&|]/\\&/g')"
            sed -i -E "s|image:[[:space:]]*ollama/ollama:latest|image: ${escaped}|g" "$out"
          fi

          if [ -n "${PRIVATE_LLM_MODEL_LOADER_IMAGE:-}" ]; then
            if ! grep -qE 'image:[[:space:]]*curlimages/curl:latest' "$out"; then
              echo "::error::Expected private-llm model-loader image line not found in $src"
              exit 1
            fi
            escaped="$(printf '%s' "${PRIVATE_LLM_MODEL_LOADER_IMAGE}" | sed -e 's/[&|]/\\&/g')"
            sed -i -E "s|image:[[:space:]]*curlimages/curl:latest|image: ${escaped}|g" "$out"
          fi

          echo "compose_file=$out" >> "$GITHUB_OUTPUT"

      - name: Deploy private-llm
        id: deploy
        uses: ./.github/actions/deploy-example
        with:
          app_name: private-llm
          compose_file: ${{ steps.llm_compose.outputs.compose_file }}
          service_name: private-llm
          health_endpoint: /
          control_plane_url: ${{ env.CP_URL }}
          agent_admin_password: ${{ secrets.AGENT_ADMIN_PASSWORD }}
          github_owner: ${{ github.repository_owner }}
          node_size: llm
          allowed_clouds: baremetal
          allowed_datacenters: baremetal:github-runner

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install SDK and OpenAI client
        run: pip install ./sdk/ openai

      - name: Smoke test private-llm (OpenAI-compatible)
        env:
          SERVICE_URL: ${{ steps.deploy.outputs.service_url }}
          EASYENCLAVE_URL: ${{ env.CP_URL }}
        shell: bash
        run: |
          set -euo pipefail
          python3 - <<'PY'
          import os
          import time

          import httpx
          import httpx as _httpx
          from easyenclave import EasyEnclaveClient
          from openai import OpenAI

          MODEL = "smollm2:135m"
          CHAT_PATH = "/v1/chat/completions"
          CHAT_BODY = {
              "model": MODEL,
              "messages": [{"role": "user", "content": "Say hello in one sentence."}],
          }
          TIMEOUT = 300
          RETRY_INTERVAL = 15

          service_url = os.environ["SERVICE_URL"].rstrip("/")
          easyenclave_url = os.environ["EASYENCLAVE_URL"].rstrip("/")

          def wait_ready(label, fn):
              deadline = time.monotonic() + TIMEOUT
              last_error = RuntimeError("unknown")
              while True:
                  try:
                      content = fn()
                      if content and content != "null":
                          print(f"[{label}] OK: {content}")
                          return
                  except Exception as exc:
                      last_error = exc
                  if time.monotonic() >= deadline:
                      raise RuntimeError(
                          f"[{label}] FAIL: not ready after {TIMEOUT}s â€” {last_error}"
                      )
                  print(f"[{label}] Model not ready, retrying in {RETRY_INTERVAL}s...")
                  time.sleep(RETRY_INTERVAL)

          def extract_content(resp: httpx.Response) -> str:
              data = resp.json()
              return data["choices"][0]["message"]["content"]

          def direct_call():
              url = f"{service_url}{CHAT_PATH}"
              with httpx.Client(timeout=60, headers={"user-agent": "EasyEnclave-Test/1.0"}) as c:
                  resp = c.post(url, json=CHAT_BODY)
                  resp.raise_for_status()
                  return extract_content(resp)

          ee = EasyEnclaveClient(easyenclave_url, verify=False)
          llm = ee.service("private-llm")

          def proxy_call():
              resp = llm.post(CHAT_PATH, json=CHAT_BODY, timeout=60)
              resp.raise_for_status()
              return extract_content(resp)

          def _strip_bot_headers(req: _httpx.Request):
              req.headers["user-agent"] = "EasyEnclave-Test/1.0"
              for key in [k for k in req.headers if k.lower().startswith("x-stainless-")]:
                  del req.headers[key]

          openai_client = OpenAI(
              base_url=f"{llm.base_url.rstrip('/')}/v1",
              api_key="unused",
              http_client=_httpx.Client(
                  verify=False,
                  event_hooks={"request": [_strip_bot_headers]},
              ),
          )

          def openai_call():
              completion = openai_client.chat.completions.create(
                  model=MODEL,
                  messages=[{"role": "user", "content": "Say hello in one sentence."}],
                  timeout=60,
              )
              return completion.choices[0].message.content or ""

          wait_ready("direct", direct_call)
          wait_ready("proxy", proxy_call)
          wait_ready("openai", openai_call)
          PY
