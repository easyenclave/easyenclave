# Builtin Deploy Examples (Baremetal)
#
# Canonical baremetal example workflow:
# - hello-tdx on tiny
# - private-llm on llm (OpenAI-compatible smoke test)

name: Builtin Deploy Examples (Baremetal)

on:
  workflow_call:
    inputs:
      cp_url:
        required: false
        type: string
        default: "https://app.easyenclave.com"
      cp_expected_git_sha:
        required: false
        type: string
        default: ""
      cp_admin_token:
        required: false
        type: string
        default: ""
      source_ref:
        required: false
        type: string
        default: ""
      hello_tdx_image:
        required: false
        type: string
        default: ""
      private_llm_ollama_image:
        required: false
        type: string
        default: ""
      private_llm_model_loader_image:
        required: false
        type: string
        default: ""
  workflow_dispatch:
    inputs:
      cp_url:
        description: "Control plane URL"
        required: false
        default: "https://app.easyenclave.com"
      cp_expected_git_sha:
        description: "Expected control-plane git SHA from /health (optional)"
        required: false
        default: ""
      cp_admin_token:
        description: "Optional admin bearer token for trust bootstrap"
        required: false
        default: ""
      source_ref:
        description: "Git ref to checkout"
        required: false
        default: ""
      hello_tdx_image:
        description: "Optional hello-tdx image override (digest ref)"
        required: false
        default: ""
      private_llm_ollama_image:
        description: "Optional private-llm ollama image override (digest ref)"
        required: false
        default: ""
      private_llm_model_loader_image:
        description: "Optional private-llm model-loader image override (digest ref)"
        required: false
        default: ""

concurrency:
  group: easyenclave-deploy-examples-baremetal-${{ github.run_id }}
  cancel-in-progress: true

env:
  CP_URL: ${{ inputs.cp_url || github.event.inputs.cp_url || 'https://app.easyenclave.com' }}
  CP_EXPECTED_GIT_SHA: ${{ inputs.cp_expected_git_sha || github.event.inputs.cp_expected_git_sha || '' }}
  CP_ADMIN_TOKEN: ${{ inputs.cp_admin_token || github.event.inputs.cp_admin_token || '' }}
  HELLO_TDX_IMAGE: ${{ inputs.hello_tdx_image || github.event.inputs.hello_tdx_image || '' }}
  PRIVATE_LLM_OLLAMA_IMAGE: ${{ inputs.private_llm_ollama_image || github.event.inputs.private_llm_ollama_image || '' }}
  PRIVATE_LLM_MODEL_LOADER_IMAGE: ${{ inputs.private_llm_model_loader_image || github.event.inputs.private_llm_model_loader_image || '' }}

jobs:
  deploy-builtin-baremetal:
    name: Deploy builtin apps (baremetal)
    runs-on: [self-hosted, tdx]
    steps:
      - name: Wait for control plane health
        shell: bash
        run: |
          set -euo pipefail
          echo "Waiting for $CP_URL/health"
          deadline=$(( $(date +%s) + 1200 ))
          while :; do
            body="$(curl -fsS "$CP_URL/health" 2>/dev/null || true)"
            if [ -z "${body:-}" ]; then
              now="$(date +%s)"
              if [ "$now" -ge "$deadline" ]; then
                echo "::error::Timed out waiting for control plane health"
                exit 1
              fi
              sleep 5
              continue
            fi

            if [ -z "${CP_EXPECTED_GIT_SHA:-}" ]; then
              echo "Control plane healthy (git_sha check skipped)"
              break
            fi

            sha="$(echo "${body:-}" | jq -r '.git_sha // empty' 2>/dev/null || true)"
            if [ -n "${sha:-}" ] && [ "$sha" = "$CP_EXPECTED_GIT_SHA" ]; then
              echo "Control plane ready: git_sha=$sha"
              break
            fi

            now="$(date +%s)"
            if [ "$now" -ge "$deadline" ]; then
              echo "::error::Timed out waiting for control plane git_sha=$CP_EXPECTED_GIT_SHA"
              echo "Last /health:"
              echo "${body:-<empty>}"
              exit 1
            fi
            sleep 5
          done

      - uses: actions/checkout@v4
        with:
          ref: ${{ inputs.source_ref || github.event.inputs.source_ref || github.sha }}

      - name: Register hello-tdx
        uses: ./.github/actions/register-app
        with:
          name: hello-tdx
          description: "Hello TDX - Example app demonstrating TDX attestation"
          control_plane_url: ${{ env.CP_URL }}

      - name: Register private-llm
        uses: ./.github/actions/register-app
        with:
          name: private-llm
          description: "Private LLM - Ollama running in a TDX enclave"
          control_plane_url: ${{ env.CP_URL }}

      - name: Prepare hello-tdx compose (optional release image override)
        id: hello_compose
        shell: bash
        run: |
          set -euo pipefail
          src="examples/hello-tdx/docker-compose.yml"
          out="${RUNNER_TEMP}/hello-tdx.compose.yml"
          cp "$src" "$out"
          if [ -n "${HELLO_TDX_IMAGE:-}" ]; then
            if ! grep -qE 'image:[[:space:]]*hashicorp/http-echo:latest' "$out"; then
              echo "::error::Expected hello-tdx image line not found in $src"
              exit 1
            fi
            escaped="$(printf '%s' "${HELLO_TDX_IMAGE}" | sed -e 's/[&|]/\\&/g')"
            sed -i -E "s|image:[[:space:]]*hashicorp/http-echo:latest|image: ${escaped}|g" "$out"
          fi
          echo "compose_file=$out" >> "$GITHUB_OUTPUT"

      - name: Deploy hello-tdx
        uses: ./.github/actions/deploy-example
        with:
          app_name: hello-tdx
          compose_file: ${{ steps.hello_compose.outputs.compose_file }}
          service_name: hello-tdx
          health_endpoint: /
          control_plane_url: ${{ env.CP_URL }}
          node_size: tiny
          allowed_clouds: baremetal
          allowed_datacenters: baremetal:github-runner

      - name: Prepare private-llm compose (optional release image overrides)
        id: llm_compose
        shell: bash
        run: |
          set -euo pipefail
          src="examples/private-llm/docker-compose.yml"
          out="${RUNNER_TEMP}/private-llm.compose.yml"
          cp "$src" "$out"

          if [ -n "${PRIVATE_LLM_OLLAMA_IMAGE:-}" ]; then
            if ! grep -qE 'image:[[:space:]]*ollama/ollama:latest' "$out"; then
              echo "::error::Expected private-llm ollama image line not found in $src"
              exit 1
            fi
            escaped="$(printf '%s' "${PRIVATE_LLM_OLLAMA_IMAGE}" | sed -e 's/[&|]/\\&/g')"
            sed -i -E "s|image:[[:space:]]*ollama/ollama:latest|image: ${escaped}|g" "$out"
          fi

          if [ -n "${PRIVATE_LLM_MODEL_LOADER_IMAGE:-}" ]; then
            if ! grep -qE 'image:[[:space:]]*curlimages/curl:latest' "$out"; then
              echo "::error::Expected private-llm model-loader image line not found in $src"
              exit 1
            fi
            escaped="$(printf '%s' "${PRIVATE_LLM_MODEL_LOADER_IMAGE}" | sed -e 's/[&|]/\\&/g')"
            sed -i -E "s|image:[[:space:]]*curlimages/curl:latest|image: ${escaped}|g" "$out"
          fi

          echo "compose_file=$out" >> "$GITHUB_OUTPUT"

      - name: Deploy private-llm
        id: deploy
        uses: ./.github/actions/deploy-example
        with:
          app_name: private-llm
          compose_file: ${{ steps.llm_compose.outputs.compose_file }}
          service_name: private-llm
          health_endpoint: /
          control_plane_url: ${{ env.CP_URL }}
          agent_admin_password: ${{ secrets.AGENT_ADMIN_PASSWORD }}
          github_owner: ${{ github.repository_owner }}
          node_size: llm
          allowed_clouds: baremetal
          allowed_datacenters: baremetal:github-runner

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install SDK and OpenAI client
        run: pip install ./sdk/ openai

      - name: Smoke test private-llm (OpenAI-compatible)
        env:
          SERVICE_URL: ${{ steps.deploy.outputs.service_url }}
          EASYENCLAVE_URL: ${{ env.CP_URL }}
        shell: bash
        run: |
          set -euo pipefail
          python3 - <<'PY'
          import os
          import time

          import httpx
          import httpx as _httpx
          from easyenclave import EasyEnclaveClient
          from openai import OpenAI

          MODEL = "smollm2:135m"
          CHAT_PATH = "/v1/chat/completions"
          CHAT_BODY = {
              "model": MODEL,
              "messages": [{"role": "user", "content": "Say hello in one sentence."}],
          }
          TIMEOUT = 300
          RETRY_INTERVAL = 15

          service_url = os.environ["SERVICE_URL"].rstrip("/")
          easyenclave_url = os.environ["EASYENCLAVE_URL"].rstrip("/")

          def wait_ready(label, fn):
              deadline = time.monotonic() + TIMEOUT
              last_error = RuntimeError("unknown")
              while True:
                  try:
                      content = fn()
                      if content and content != "null":
                          print(f"[{label}] OK: {content}")
                          return
                  except Exception as exc:
                      last_error = exc
                  if time.monotonic() >= deadline:
                      raise RuntimeError(
                          f"[{label}] FAIL: not ready after {TIMEOUT}s â€” {last_error}"
                      )
                  print(f"[{label}] Model not ready, retrying in {RETRY_INTERVAL}s...")
                  time.sleep(RETRY_INTERVAL)

          def extract_content(resp: httpx.Response) -> str:
              data = resp.json()
              return data["choices"][0]["message"]["content"]

          def direct_call():
              url = f"{service_url}{CHAT_PATH}"
              with httpx.Client(timeout=60, headers={"user-agent": "EasyEnclave-Test/1.0"}) as c:
                  resp = c.post(url, json=CHAT_BODY)
                  resp.raise_for_status()
                  return extract_content(resp)

          ee = EasyEnclaveClient(easyenclave_url, verify=False)
          llm = ee.service("private-llm")

          def proxy_call():
              resp = llm.post(CHAT_PATH, json=CHAT_BODY, timeout=60)
              resp.raise_for_status()
              return extract_content(resp)

          def _strip_bot_headers(req: _httpx.Request):
              req.headers["user-agent"] = "EasyEnclave-Test/1.0"
              for key in [k for k in req.headers if k.lower().startswith("x-stainless-")]:
                  del req.headers[key]

          openai_client = OpenAI(
              base_url=f"{llm.base_url.rstrip('/')}/v1",
              api_key="unused",
              http_client=_httpx.Client(
                  verify=False,
                  event_hooks={"request": [_strip_bot_headers]},
              ),
          )

          def openai_call():
              completion = openai_client.chat.completions.create(
                  model=MODEL,
                  messages=[{"role": "user", "content": "Say hello in one sentence."}],
                  timeout=60,
              )
              return completion.choices[0].message.content or ""

          wait_ready("direct", direct_call)
          wait_ready("proxy", proxy_call)
          wait_ready("openai", openai_call)
          PY
